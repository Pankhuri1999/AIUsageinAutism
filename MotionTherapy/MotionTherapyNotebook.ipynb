{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9b812f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== FINAL OUTCOME ====================\n",
      "Video 1: degree1.mp4\n",
      "  Frames scanned: 186\n",
      "  Pose frames: 186\n",
      "  Pose detection rate (%): 100.00\n",
      "  Avg latency (ms/frame): 123.87\n",
      "  P95 latency (ms/frame): 161.45\n",
      "\n",
      "Video 2: degree2.mp4\n",
      "  Frames scanned: 174\n",
      "  Pose frames: 174\n",
      "  Pose detection rate (%): 100.00\n",
      "  Avg latency (ms/frame): 172.02\n",
      "  P95 latency (ms/frame): 369.54\n",
      "\n",
      "Motion series lengths:\n",
      "  motion1_len: 185\n",
      "  motion2_len: 173\n",
      "\n",
      "FastDTW:\n",
      "  distance (lower=more similar): 108.368277\n",
      "  alignment path len (total frame-pairs compared): 244\n",
      "  unique frames compared (video1): 185 (100.0%)\n",
      "  unique frames compared (video2): 173 (100.0%)\n",
      "  normalized DTW cost (distance/path_len): 0.44413228\n",
      "\n",
      "Similarity (single pair, 1..10): 3.375\n",
      "=======================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from fastdtw import fastdtw\n",
    "\n",
    "\n",
    "\n",
    "mp_pose = mp.solutions.pose.Pose(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=1,\n",
    "    enable_segmentation=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def extract_pose_sequence_with_stats(video_path):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      pose_sequence: list of (33,2) arrays (only frames where pose detected)\n",
    "      stats: dict with frames scanned, pose frames, latency etc.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    pose_sequence = []\n",
    "    total_frames = 0\n",
    "    pose_frames = 0\n",
    "    frame_times = []  \n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        total_frames += 1\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        res = mp_pose.process(rgb)\n",
    "        t1 = time.perf_counter()\n",
    "\n",
    "        frame_times.append(t1 - t0)\n",
    "\n",
    "        if res.pose_landmarks:\n",
    "            pose_frames += 1\n",
    "            keypoints = np.array([[lm.x, lm.y] for lm in res.pose_landmarks.landmark], dtype=np.float32)\n",
    "            pose_sequence.append(keypoints)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    avg_ms = 1000 * (np.mean(frame_times) if frame_times else float(\"nan\"))\n",
    "    p95_ms = 1000 * (np.percentile(frame_times, 95) if frame_times else float(\"nan\"))\n",
    "\n",
    "    stats = {\n",
    "        \"video\": video_path,\n",
    "        \"total_frames_scanned\": total_frames,\n",
    "        \"frames_with_pose\": pose_frames,\n",
    "        \"pose_detection_rate_%\": 100 * pose_frames / max(total_frames, 1),\n",
    "        \"avg_processing_ms_per_frame\": avg_ms,\n",
    "        \"p95_processing_ms_per_frame\": p95_ms,\n",
    "    }\n",
    "    return pose_sequence, stats\n",
    "\n",
    "\n",
    "\n",
    "def pose_to_motion_series(pose_seq):\n",
    "    \"\"\"\n",
    "    1D motion signal: mean joint displacement between consecutive pose frames.\n",
    "    Output length = len(pose_seq)-1\n",
    "    \"\"\"\n",
    "    if len(pose_seq) < 2:\n",
    "        return np.array([], dtype=np.float32)\n",
    "\n",
    "    motion = []\n",
    "    for i in range(1, len(pose_seq)):\n",
    "        prev = pose_seq[i - 1]\n",
    "        curr = pose_seq[i]\n",
    "        disp = np.linalg.norm(curr - prev, axis=1)  # (33,)\n",
    "        motion.append(float(np.mean(disp)))\n",
    "    return np.array(motion, dtype=np.float32)\n",
    "\n",
    "\n",
    "def z_normalize(x):\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    if len(x) == 0:\n",
    "        return x\n",
    "    return (x - x.mean()) / (x.std() + 1e-9)\n",
    "\n",
    "\n",
    "\n",
    "def compare_two_videos(video1, video2, normalize_series=True):\n",
    "    \"\"\"\n",
    "    Returns a dict with all outcomes + motion series + dtw path.\n",
    "    \"\"\"\n",
    "    pose1, stats1 = extract_pose_sequence_with_stats(video1)\n",
    "    pose2, stats2 = extract_pose_sequence_with_stats(video2)\n",
    "\n",
    "    motion1 = pose_to_motion_series(pose1)\n",
    "    motion2 = pose_to_motion_series(pose2)\n",
    "\n",
    "    if normalize_series:\n",
    "        motion1 = z_normalize(motion1)\n",
    "        motion2 = z_normalize(motion2)\n",
    "\n",
    "    if len(motion1) == 0 or len(motion2) == 0:\n",
    "        return {\n",
    "            \"error\": \"Not enough pose frames to compute motion series for one or both videos.\",\n",
    "            \"stats1\": stats1,\n",
    "            \"stats2\": stats2,\n",
    "            \"motion1_len\": len(motion1),\n",
    "            \"motion2_len\": len(motion2),\n",
    "        }, motion1, motion2, []\n",
    "\n",
    "\n",
    "    dtw_distance, path = fastdtw(motion1, motion2, dist=lambda a, b: abs(a - b))\n",
    "\n",
    "    path_len = len(path)  \n",
    "    unique_v1 = len(set(i for i, j in path))\n",
    "    unique_v2 = len(set(j for i, j in path))\n",
    "\n",
    "    cov_v1 = 100 * unique_v1 / max(len(motion1), 1)\n",
    "    cov_v2 = 100 * unique_v2 / max(len(motion2), 1)\n",
    "\n",
    "    dtw_cost = float(dtw_distance) / (path_len + 1e-9) \n",
    "\n",
    "    result = {\n",
    "        \"stats_video1\": stats1,\n",
    "        \"stats_video2\": stats2,\n",
    "        \"motion1_len\": int(len(motion1)),\n",
    "        \"motion2_len\": int(len(motion2)),\n",
    "        \"fastdtw_distance\": float(dtw_distance),\n",
    "        \"alignment_path_len_total_pairs\": int(path_len),\n",
    "        \"unique_frames_compared_video1\": int(unique_v1),\n",
    "        \"unique_frames_compared_video2\": int(unique_v2),\n",
    "        \"coverage_%_video1\": float(cov_v1),\n",
    "        \"coverage_%_video2\": float(cov_v2),\n",
    "        \"dtw_cost_distance_per_step\": float(dtw_cost),\n",
    " \n",
    "    }\n",
    "    return result, motion1, motion2, path\n",
    "\n",
    "\n",
    "\n",
    "def distance_to_similarity_single_pair(d, alpha=1.0):\n",
    "    \"\"\"\n",
    "    For a SINGLE pair (no dataset min/max), use a smooth inverse mapping:\n",
    "      sim01 = exp(-alpha * d)\n",
    "      sim_1_10 = 1 + 9*sim01\n",
    "    Lower d => higher similarity.\n",
    "    alpha controls how fast similarity drops with distance.\n",
    "    \"\"\"\n",
    "    d = float(d)\n",
    "    sim01 = np.exp(-alpha * d)\n",
    "    return float(1 + 9 * sim01)\n",
    "\n",
    "\n",
    "\n",
    "def plot_motion_overlap(m1, m2, title=\"Motion overlap (normalized)\"):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(m1, label=\"Video 1 motion\")\n",
    "    plt.plot(m2, label=\"Video 2 motion\")\n",
    "    plt.xlabel(\"Time index\")\n",
    "    plt.ylabel(\"Motion (normalized)\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_dtw_alignment_path(path, title=\"FastDTW alignment path\"):\n",
    "    if not path:\n",
    "        print(\"No DTW path to plot.\")\n",
    "        return\n",
    "    xs = [p[0] for p in path]\n",
    "    ys = [p[1] for p in path]\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(xs, ys, linewidth=1)\n",
    "    plt.xlabel(\"Video 1 time index\")\n",
    "    plt.ylabel(\"Video 2 time index\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    VIDEO1_PATH = \"degree1.mp4\"\n",
    "    VIDEO2_PATH = \"degree2.mp4\"\n",
    "\n",
    "    result, motion1, motion2, path = compare_two_videos(VIDEO1_PATH, VIDEO2_PATH, normalize_series=True)\n",
    "\n",
    "    if \"error\" in result:\n",
    "        print(\"ERROR:\", result[\"error\"])\n",
    "        print(\"Video1 stats:\", result[\"stats1\"])\n",
    "        print(\"Video2 stats:\", result[\"stats2\"])\n",
    "    else:\n",
    "        # Single-pair similarity score (1..10) using exp mapping\n",
    "        sim_1_10 = distance_to_similarity_single_pair(result[\"dtw_cost_distance_per_step\"], alpha=3.0)\n",
    "        result[\"similarity_1_to_10_single_pair\"] = sim_1_10\n",
    "\n",
    "        print(\"\\n==================== FINAL OUTCOME ====================\")\n",
    "        print(\"Video 1:\", result[\"stats_video1\"][\"video\"])\n",
    "        print(\"  Frames scanned:\", result[\"stats_video1\"][\"total_frames_scanned\"])\n",
    "        print(\"  Pose frames:\", result[\"stats_video1\"][\"frames_with_pose\"])\n",
    "        print(\"  Pose detection rate (%):\", f'{result[\"stats_video1\"][\"pose_detection_rate_%\"]:.2f}')\n",
    "        print(\"  Avg latency (ms/frame):\", f'{result[\"stats_video1\"][\"avg_processing_ms_per_frame\"]:.2f}')\n",
    "        print(\"  P95 latency (ms/frame):\", f'{result[\"stats_video1\"][\"p95_processing_ms_per_frame\"]:.2f}')\n",
    "\n",
    "        print(\"\\nVideo 2:\", result[\"stats_video2\"][\"video\"])\n",
    "        print(\"  Frames scanned:\", result[\"stats_video2\"][\"total_frames_scanned\"])\n",
    "        print(\"  Pose frames:\", result[\"stats_video2\"][\"frames_with_pose\"])\n",
    "        print(\"  Pose detection rate (%):\", f'{result[\"stats_video2\"][\"pose_detection_rate_%\"]:.2f}')\n",
    "        print(\"  Avg latency (ms/frame):\", f'{result[\"stats_video2\"][\"avg_processing_ms_per_frame\"]:.2f}')\n",
    "        print(\"  P95 latency (ms/frame):\", f'{result[\"stats_video2\"][\"p95_processing_ms_per_frame\"]:.2f}')\n",
    "\n",
    "        print(\"\\nMotion series lengths:\")\n",
    "        print(\"  motion1_len:\", result[\"motion1_len\"])\n",
    "        print(\"  motion2_len:\", result[\"motion2_len\"])\n",
    "\n",
    "        print(\"\\nFastDTW:\")\n",
    "        print(\"  distance (lower=more similar):\", f'{result[\"fastdtw_distance\"]:.6f}')\n",
    "        print(\"  alignment path len (total frame-pairs compared):\", result[\"alignment_path_len_total_pairs\"])\n",
    "        print(\"  unique frames compared (video1):\", result[\"unique_frames_compared_video1\"],\n",
    "              f'({result[\"coverage_%_video1\"]:.1f}%)')\n",
    "        print(\"  unique frames compared (video2):\", result[\"unique_frames_compared_video2\"],\n",
    "              f'({result[\"coverage_%_video2\"]:.1f}%)')\n",
    "        print(\"  normalized DTW cost (distance/path_len):\", f'{result[\"dtw_cost_distance_per_step\"]:.8f}')\n",
    "\n",
    "        print(\"\\nSimilarity (single pair, 1..10):\", f'{result[\"similarity_1_to_10_single_pair\"]:.3f}')\n",
    "        print(\"=======================================================\\n\")\n",
    "\n",
    "\n",
    "        plot_motion_overlap(motion1, motion2, title=\"Motion overlap (z-normalized)\")\n",
    "        plot_dtw_alignment_path(path, title=\"FastDTW alignment path\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b4fefd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_mp_new)",
   "language": "python",
   "name": "tf_mp_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
